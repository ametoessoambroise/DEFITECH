# DefAI - Assistant IA pour Plateforme Universitaire

DefAI est un assistant intelligent basÃ© sur Google Gemma 2B, fine-tunÃ© spÃ©cifiquement pour les besoins d'une plateforme universitaire. Il peut analyser les comportements utilisateurs, assister le personnel, et s'intÃ©grer parfaitement dans l'Ã©cosystÃ¨me existant.

## ğŸ¯ Objectifs

- **Analyse comportementale** : Comprendre les parcours utilisateurs et optimiser l'expÃ©rience
- **Assistance intelligente** : Aider les Ã©tudiants, enseignants et administrateurs
- **DÃ©bogage proactif** : Identifier et rÃ©soudre les problÃ¨mes rapidement
- **IntÃ©gration transparente** : S'intÃ©grer dans les systÃ¨mes existants via API REST

## ğŸ“ Structure du Projet

```
fineTuning_defAI/
â”œâ”€â”€ config/                     # Fichiers de configuration
â”‚   â”œâ”€â”€ training_config.yaml    # Configuration d'entraÃ®nement
â”‚   â””â”€â”€ model_config.yaml       # Configuration du modÃ¨le
â”œâ”€â”€ data/                       # DonnÃ©es d'entraÃ®nement
â”‚   â”œâ”€â”€ raw/                    # DonnÃ©es brutes extraites
â”‚   â”œâ”€â”€ formatted/              # DonnÃ©es formatÃ©es pour le training
â”‚   â””â”€â”€ synthetic/              # DonnÃ©es synthÃ©tiques gÃ©nÃ©rÃ©es
â”œâ”€â”€ scripts/                    # Scripts Python
â”‚   â”œâ”€â”€ extract_db_data.py      # Extraction PostgreSQL
â”‚   â”œâ”€â”€ track_routes.py         # Middleware Flask de tracking
â”‚   â”œâ”€â”€ generate_dataset.py     # GÃ©nÃ©ration du dataset
â”‚   â”œâ”€â”€ train.py                # Fine-tuning Gemma 2B
â”‚   â””â”€â”€ evaluate.py             # Ã‰valuation du modÃ¨le
â”œâ”€â”€ model/                      # ModÃ¨les
â”‚   â”œâ”€â”€ gemma_base/            # Gemma 2B de base
â”‚   â””â”€â”€ gemma_finetuned/       # DefAI fine-tunÃ©
â”œâ”€â”€ deployment/                 # DÃ©ploiement
â”‚   â”œâ”€â”€ inference.py           # API d'infÃ©rence
â”‚   â””â”€â”€ routes_middleware.py   # Middleware d'intÃ©gration
â”œâ”€â”€ tests/                      # Tests unitaires
â””â”€â”€ README.md                   # Ce fichier
```

## ğŸš€ Installation et Configuration

### PrÃ©requis

- Python 3.9+
- PostgreSQL (pour l'extraction de donnÃ©es)
- GPU NVIDIA (recommandÃ© pour le fine-tuning)
- 16GB+ RAM (minimum)

### Installation des DÃ©pendances

```bash
# Cloner le projet
git clone <https://github.com/Smiler00/fineTuning_defAI.git>
cd fineTuning_defAI

# CrÃ©er l'environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# Installer les dÃ©pendances
pip install -r requirements.txt
```

### Configuration de l'Environnement

CrÃ©er un fichier `.env` Ã  la racine :

```bash
# Configuration PostgreSQL
DB_HOST=localhost
DB_PORT=5432
DB_NAME=university_db
DB_USER=postgres
DB_PASSWORD=your_password

# Configuration DefAI
DEFAI_URL=http://localhost:5000
DEFAI_TIMEOUT=30
DEFAI_ENABLED=true

# Configuration HuggingFace
HF_TOKEN=your_huggingface_token
```

## ğŸ“Š Ã‰tape 1 : Extraction des DonnÃ©es

Extrayez les donnÃ©es de votre base PostgreSQL existante :

```bash
cd scripts
python extract_db_data.py
```

Ce script :

- Se connecte Ã  PostgreSQL
- Extrait les donnÃ©es des tables : users, teachers, students, courses, grades, user_routes_logs
- Sanitise les informations sensibles (emails, mots de passe, tÃ©lÃ©phones)
- Exporte en format JSON Lines dans `data/raw/`

## ğŸ›£ï¸ Ã‰tape 2 : Tracking des Routes

IntÃ©grez le middleware Flask dans votre application existante :

```python
from track_routes import RouteTracker

app = Flask(__name__)
tracker = RouteTracker(app)

# Le middleware capturera automatiquement toutes les routes
```

Ou utilisez-le comme dÃ©corateur :

```python
@app.route('/dashboard')
@track_route()
def dashboard():
    return render_template('dashboard.html')
```

Le tracker enregistre :

- Route et mÃ©thode HTTP
- Informations utilisateur
- DurÃ©e de traitement
- Codes d'erreur
- MÃ©tadonnÃ©es contextuelles

## ğŸ§  Ã‰tape 3 : GÃ©nÃ©ration du Dataset

GÃ©nÃ©rez le dataset d'entraÃ®nement combinant donnÃ©es rÃ©elles et synthÃ©tiques :

```bash
cd scripts
python generate_dataset.py
```

Ce script :

- Charge les donnÃ©es brutes
- GÃ©nÃ¨re des exemples synthÃ©tiques pour :
  - Assistance utilisateur
  - Analyse de routes
  - RÃ©solution d'erreurs
  - TÃ¢ches administratives
- CrÃ©e les splits train/valid/test (80%/15%/5%)
- Sauvegarde en JSON Lines dans `data/formatted/`

## ğŸ‹ï¸ Ã‰tape 4 : Fine-Tuning du ModÃ¨le

EntraÃ®nez DefAI avec Gemma 2B et LoRA/QLoRA :

```bash
cd scripts
python train.py
```

Configuration via `config/training_config.yaml` :

- ModÃ¨le : Google Gemma 2B
- Technique : LoRA/QLoRA (quantification 4-bit)
- HyperparamÃ¨tres optimisÃ©s
- Monitoring automatique

**HyperparamÃ¨tres par dÃ©faut :**

- Learning rate : 2e-5
- Batch size : 2
- Ã‰poques : 3-5
- Max sequence length : 2048
- LoRA rank : 16

## ğŸ“ˆ Ã‰tape 5 : Ã‰valuation

Ã‰valuez les performances du modÃ¨le fine-tunÃ© :

```bash
cd scripts
python evaluate.py
```

MÃ©triques Ã©valuÃ©es :

- BLEU, ROUGE-1/2/L
- PerplexitÃ©
- TÃ¢ches spÃ©cifiques DefAI
- Comparaison avec modÃ¨le de base

RÃ©sultats sauvegardÃ©s dans `evaluation_results/`

## ğŸš€ Ã‰tape 6 : DÃ©ploiement

### API d'InfÃ©rence

DÃ©marrez l'API DefAI :

```bash
cd deployment
python inference.py --host 0.0.0.0 --port 5555
```

Endpoints disponibles :

- `POST /chat` - Chat avec l'IA
- `POST /analyze` - Analyse de situation
- `POST /assist` - Assistance utilisateur
- `POST /routes/suggest` - Suggestions de routes
- `POST /debug/error` - Aide au dÃ©bogage
- `GET /health` - VÃ©rification de l'Ã©tat

### IntÃ©gration Middleware

IntÃ©grez DefAI dans votre application existante :

```python
from routes_middleware import create_defai_app

# CrÃ©er l'application avec DefAI intÃ©grÃ©
app = create_defai_app()

# Ou ajouter Ã  une application existante
from routes_middleware import DefAIMiddleware
middleware = DefAIMiddleware(app)
```

## ğŸ“ Utilisation

### Chat avec DefAI

```python
import requests

response = requests.post('http://localhost:5555/chat', json={
    'message': 'Comment un Ã©tudiant peut-il consulter ses notes ?'
})

print(response.json()['response'])
```

### Assistance Utilisateur

```python
response = requests.post('http://localhost:5000/assist', json={
    'query': 'Je ne peux pas accÃ©der Ã  mes cours',
    'user_role': 'student'
})
```

### DÃ©bogage d'Erreurs

```python
response = requests.post('http://localhost:5000/debug/error', json={
    'error_info': 'Erreur 500 lors de la soumission du formulaire'
})
```

## ğŸ”§ Configuration AvancÃ©e

### Customisation du Dataset

Modifiez `scripts/generate_dataset.py` pour ajouter vos propres templates :

```python
# Ajouter de nouveaux templates
custom_templates = [
    {
        'instruction': 'Comment {action} dans {module} ?',
        'response': 'Pour {action} dans {module}, vous devez...',
        'variables': ['action', 'module']
    }
]
```

### HyperparamÃ¨tres

Ajustez `config/training_config.yaml` :

```yaml
training:
  learning_rate: 1e-5 # Plus conservateur
  batch_size: 4 # Plus grand batch
  num_epochs: 10 # Plus d'Ã©poques

lora:
  r: 32 # Rank plus Ã©levÃ©
  alpha: 64 # Alpha plus Ã©levÃ©
```

### Production

Configuration Docker pour la production :

```dockerfile
FROM python:3.9-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
EXPOSE 5000

CMD ["python", "deployment/inference.py"]
```

## ğŸ§ª Tests

ExÃ©cutez les tests :

```bash
python -m pytest tests/ -v
```

Tests disponibles :

- Tests unitaires pour chaque script
- Tests d'intÃ©gration API
- Tests de performance modÃ¨le

## ğŸ“Š Monitoring

### Logs

Les logs sont configurÃ©s pour chaque composant :

- Niveau INFO par dÃ©faut
- Fichiers de log dans `logs/`
- Rotation automatique

### MÃ©triques

Surveillez les performances avec :

- Temps de rÃ©ponse API
- Utilisation GPU/CPU
- QualitÃ© des rÃ©ponses
- Taux d'erreur

## ğŸ”’ SÃ©curitÃ©

### Protection des DonnÃ©es

- Sanitisation automatique des donnÃ©es sensibles
- Masquage des emails, mots de passe, tÃ©lÃ©phones
- Chiffrement en transit (HTTPS)

### SÃ©curitÃ© API

- CORS configurÃ©
- Validation des entrÃ©es
- Rate limiting recommandÃ©
- Authentification Ã  implÃ©menter

## ğŸš¨ DÃ©pannage

### ProblÃ¨mes Communs

**GPU non reconnu :**

```bash
# VÃ©rifier CUDA
nvidia-smi
torch.cuda.is_available()
```

**MÃ©moire insuffisante :**

- RÃ©duire batch_size
- Activer gradient_checkpointing
- Utiliser quantification 4-bit

**ModÃ¨le ne se charge pas :**

- VÃ©rifier les chemins dans les configs
- Confirmer les permissions fichiers
- RedÃ©marrer le service

### Support

Pour obtenir de l'aide :

1. Consulter les logs dans `logs/`
2. VÃ©rifier la configuration `.env`
3. Tester avec le dataset minimal
4. CrÃ©er une issue sur GitHub

## ğŸ“ˆ Performance

### Benchmarks

Sur GPU RTX 3080 :

- Extraction donnÃ©es : ~2 minutes
- GÃ©nÃ©ration dataset : ~5 minutes
- Fine-tuning : ~2 heures
- InfÃ©rence : ~100ms par requÃªte

### Optimisations

- Utiliser CUDA pour le fine-tuning
- Quantification 4-bit pour rÃ©duire mÃ©moire
- Batch size optimal selon GPU
- Cache des rÃ©ponses frÃ©quentes

## ğŸ”„ Mises Ã  Jour

### Versionning

- Tags Git pour chaque version
- Changelog dÃ©taillÃ©
- Tests de rÃ©gression
- Migration des configs

### AmÃ©liorations Futures

- Support multi-langues
- Interface web admin
- Dashboard analytics
- IntÃ©gration avec plus de systÃ¨mes

## ğŸ“„ Licence

Ce projet est sous licence MIT - voir le fichier LICENSE pour les dÃ©tails.

## ğŸ¤ Contribution

Les contributions sont bienvenues !

1. Fork le projet
2. CrÃ©er une branche feature
3. Faire les modifications
4. Ajouter des tests
5. Soumettre une PR

## ğŸ“ Contact

Pour toute question ou suggestion :

- CrÃ©er une issue sur GitHub
- Contacter l'Ã©quipe de dÃ©veloppement
- Consulter la documentation technique

---

**DefAI** - L'assistant intelligent qui transforme votre plateforme universitaire ğŸš€
