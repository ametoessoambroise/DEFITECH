# =============================================
#   CONFIGURATION FINE-TUNING DEFAI - GEMMA 2B
# =============================================

model:
  name: "google/gemma-2b"
  tokenizer: "google/gemma-2b"

  # Limite CPU/training léger
  max_seq_length: 2048
  dtype: "float16"

  # Désactivation explicite du full precision pour CPU
  low_cpu_mem_usage: true
  trust_remote_code: true

training:
  # -----------------------------
  # Paramètres d'entraînement
  # -----------------------------
  learning_rate: 2e-5
  batch_size: 2
  gradient_accumulation_steps: 8 # Augmenté pour CPU
  num_epochs: 5
  warmup_steps: 200
  max_steps: -1

  # -----------------------------
  # Chemins de sortie
  # -----------------------------
  output_dir: "../model/gemma2b_defai"
  save_steps: 300
  eval_steps: 300
  logging_steps: 50

  # -----------------------------
  # Optimisation
  # -----------------------------
  optimizer: "adamw_torch"
  scheduler: "cosine"
  weight_decay: 0.01
  max_grad_norm: 1.0
  gradient_checkpointing: true # Réduit drastiquement la RAM CPU

  # -----------------------------
  # LoRA / QLoRA
  # -----------------------------
  lora:
    enabled: true
    r: 32 # un peu plus élevé → meilleur apprentissage
    lora_alpha: 32
    dropout: 0.05
    bias: "none"
    task_type: "CAUSAL_LM"
    target_modules:
      - "q_proj"
      - "k_proj"
      - "v_proj"
      - "o_proj"
      - "gate_proj"
      - "up_proj"
      - "down_proj"

  quantization:
    enabled: true
    bits: 4
    double_quant: true
    quant_type: "nf4" # optimal pour QLoRA CPU

data:
  # -----------------------------
  # Emplacements des données
  # -----------------------------
  train_file: "../data/formatted/train.jsonl"
  valid_file: "../data/formatted/valid.jsonl"
  test_file: "../data/formatted/test.jsonl"

  # -----------------------------
  # Split automatique
  # -----------------------------
  train_split: 0.8
  valid_split: 0.15
  test_split: 0.05

  # -----------------------------
  # Pré-traitement
  # -----------------------------
  max_length: 512
  truncation: true
  padding: "max_length"
  remove_empty_samples: true

evaluation:
  enabled: true
  eval_steps: 300
  save_best_model: true
  metrics:
    - "perplexity"
    - "bleu"
    - "rouge"
    - "accuracy" # utile pour les questions étudiantes

hardware:
  device: "auto" # auto CPU/MPS/CUDA
  mixed_precision: true
  num_workers: 4
  cpu_threads: 8 # important pour CPU
  pin_memory: false # inutile sur CPU, on désactive

logging:
  project_name: "defai-finetuning"
  experiment_name: "gemma2b-university"
  log_level: "INFO"
  use_wandb: false
  save_logs: true
  log_to_file: "../logs/training.log"
